{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1bb1cdd3",
      "metadata": {
        "id": "1bb1cdd3",
        "papermill": {
          "duration": 0.011208,
          "end_time": "2025-11-17T07:52:55.860084",
          "exception": false,
          "start_time": "2025-11-17T07:52:55.848876",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# ENVIRONMENT SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70240ab5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:52:55.880686Z",
          "iopub.status.busy": "2025-11-17T07:52:55.880340Z",
          "iopub.status.idle": "2025-11-17T07:54:47.576130Z",
          "shell.execute_reply": "2025-11-17T07:54:47.575071Z"
        },
        "id": "70240ab5",
        "papermill": {
          "duration": 111.708851,
          "end_time": "2025-11-17T07:54:47.578501",
          "exception": false,
          "start_time": "2025-11-17T07:52:55.869650",
          "status": "completed"
        },
        "tags": [],
        "outputId": "9d964621-726f-4486-f624-652b248b3c60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
            "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
            "pathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\r\n",
            "pathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\r\n",
            "s3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2023.10.0 which is incompatible.\r\n",
            "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
            "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\r\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\r\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\r\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\r\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\r\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\r\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\r\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\r\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\r\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\r\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\r\n",
            "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\r\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m330.9/330.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
            "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
            "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
            "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers==4.45.0\n",
        "!pip install -q datasets==2.16.1\n",
        "!pip install -q accelerate==1.0.0\n",
        "!pip install -q evaluate==0.4.3\n",
        "!pip install -q jiwer==3.0.3\n",
        "!pip install -q tensorboard\n",
        "!pip install -q soundfile\n",
        "!pip install -q librosa"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ae2dcb6",
      "metadata": {
        "id": "0ae2dcb6",
        "papermill": {
          "duration": 0.027361,
          "end_time": "2025-11-17T07:54:47.633648",
          "exception": false,
          "start_time": "2025-11-17T07:54:47.606287",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# SYSTEM VERIFICATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac6be01b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:54:47.689577Z",
          "iopub.status.busy": "2025-11-17T07:54:47.689276Z",
          "iopub.status.idle": "2025-11-17T07:54:51.730886Z",
          "shell.execute_reply": "2025-11-17T07:54:51.729988Z"
        },
        "id": "ac6be01b",
        "papermill": {
          "duration": 4.071373,
          "end_time": "2025-11-17T07:54:51.732154",
          "exception": false,
          "start_time": "2025-11-17T07:54:47.660781",
          "status": "completed"
        },
        "tags": [],
        "outputId": "cbe7d9a4-19c6-4ca0-988a-1def5fa7aac4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "SYSTEM VERIFICATION\n",
            "======================================================================\n",
            "\n",
            "ğŸ”§ GPU Configuration:\n",
            "   CUDA_VISIBLE_DEVICES: 0\n",
            "   Current Device: 0\n",
            "\n",
            "ğŸ“¦ Library Versions:\n",
            "   PyTorch: 2.6.0+cu124\n",
            "   Transformers: 4.45.0\n",
            "   Accelerate: 1.0.0\n",
            "\n",
            "ğŸ–¥ï¸ Hardware Status:\n",
            "   CUDA Available: True\n",
            "   Number of GPUs Visible: 1\n",
            "   GPU Name: Tesla T4\n",
            "   GPU Memory: 14.74 GB\n",
            "\n",
            "âœ… All systems verified and memory cleared\n",
            "\n",
            "======================================================================\n",
            "DETAILED GPU INFORMATION\n",
            "======================================================================\n",
            "\n",
            "Mon Nov 17 07:54:51 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   43C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "import transformers\n",
        "import accelerate\n",
        "import gc\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"SYSTEM VERIFICATION\")\n",
        "print(\"=\" * 70)\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "torch.cuda.set_device(0)\n",
        "\n",
        "print(\"\\nğŸ”§ GPU Configuration:\")\n",
        "print(f\"   CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}\")\n",
        "print(f\"   Current Device: {torch.cuda.current_device()}\")\n",
        "print(f\"\\nğŸ“¦ Library Versions:\")\n",
        "print(f\"   PyTorch: {torch.__version__}\")\n",
        "print(f\"   Transformers: {transformers.__version__}\")\n",
        "print(f\"   Accelerate: {accelerate.__version__}\")\n",
        "\n",
        "print(f\"\\nğŸ–¥ï¸ Hardware Status:\")\n",
        "print(f\"   CUDA Available: {torch.cuda.is_available()}\")\n",
        "print(f\"   Number of GPUs Visible: {torch.cuda.device_count()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nâœ… All systems verified and memory cleared\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DETAILED GPU INFORMATION\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "\n",
        "if gpu_info.find('failed') >= 0:\n",
        "    print('âŒ Not connected to a GPU')\n",
        "else:\n",
        "    print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9adabec",
      "metadata": {
        "id": "b9adabec",
        "papermill": {
          "duration": 0.027581,
          "end_time": "2025-11-17T07:54:51.788285",
          "exception": false,
          "start_time": "2025-11-17T07:54:51.760704",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# DATA PREPARATION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9cec58b",
      "metadata": {
        "id": "b9cec58b",
        "papermill": {
          "duration": 0.028062,
          "end_time": "2025-11-17T07:54:51.843787",
          "exception": false,
          "start_time": "2025-11-17T07:54:51.815725",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Download and Extract Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04502eea",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:54:51.900256Z",
          "iopub.status.busy": "2025-11-17T07:54:51.899856Z",
          "iopub.status.idle": "2025-11-17T07:54:58.901291Z",
          "shell.execute_reply": "2025-11-17T07:54:58.900502Z"
        },
        "id": "04502eea",
        "papermill": {
          "duration": 7.031308,
          "end_time": "2025-11-17T07:54:58.902571",
          "exception": false,
          "start_time": "2025-11-17T07:54:51.871263",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "5551de02c1f74042bfc530125d54de83",
            "94ecd563e471486fb3cc8f15d1db651a",
            "4802da8dbe53487b87023d960de63485",
            "ba4d3748124e4323ae858706f8eddbca",
            "8792d573a0bf4df186bdecf74f9aed54"
          ]
        },
        "outputId": "0e10d5aa-e585-469b-c476-5bbabe558c23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "DATA PREPARATION\n",
            "======================================================================\n",
            "\n",
            "ğŸ“¥ Downloading dataset from Hugging Face...\n",
            "ğŸ“ Found 5 files in the repository\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5551de02c1f74042bfc530125d54de83",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              ".gitattributes: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94ecd563e471486fb3cc8f15d1db651a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4802da8dbe53487b87023d960de63485",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Test.zip:   0%|          | 0.00/63.0M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba4d3748124e4323ae858706f8eddbca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Train.zip:   0%|          | 0.00/289M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8792d573a0bf4df186bdecf74f9aed54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Train_annotation.zip:   0%|          | 0.00/58.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… All files downloaded to: ./\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DATA PREPARATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Install huggingface_hub for dataset download\n",
        "!pip install -q huggingface_hub\n",
        "\n",
        "from huggingface_hub import list_repo_files, hf_hub_download\n",
        "import shutil\n",
        "\n",
        "print(\"\\nğŸ“¥ Downloading dataset from Hugging Face...\")\n",
        "\n",
        "# Repository configuration\n",
        "repo_id = \"bitwisemind/hackathon\"\n",
        "repo_type = \"dataset\"\n",
        "download_dir = \"./\"\n",
        "\n",
        "# List and download all files\n",
        "files = list_repo_files(repo_id=repo_id, repo_type=repo_type)\n",
        "print(f\"ğŸ“ Found {len(files)} files in the repository\")\n",
        "\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "downloaded_files = []\n",
        "\n",
        "for file in files:\n",
        "    file_path = hf_hub_download(repo_id=repo_id, filename=file, repo_type=repo_type)\n",
        "    local_path = os.path.join(download_dir, file)\n",
        "    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "    shutil.copy(file_path, local_path)\n",
        "    downloaded_files.append(local_path)\n",
        "\n",
        "print(f\"âœ… All files downloaded to: {download_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13567ea8",
      "metadata": {
        "id": "13567ea8",
        "papermill": {
          "duration": 0.0274,
          "end_time": "2025-11-17T07:54:58.958296",
          "exception": false,
          "start_time": "2025-11-17T07:54:58.930896",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Extract Downloaded Archives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e64b77c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:54:59.014004Z",
          "iopub.status.busy": "2025-11-17T07:54:59.013773Z",
          "iopub.status.idle": "2025-11-17T07:55:04.709649Z",
          "shell.execute_reply": "2025-11-17T07:55:04.708622Z"
        },
        "id": "0e64b77c",
        "papermill": {
          "duration": 5.725247,
          "end_time": "2025-11-17T07:55:04.710858",
          "exception": false,
          "start_time": "2025-11-17T07:54:58.985611",
          "status": "completed"
        },
        "tags": [],
        "outputId": "3a9cf025-8425-4a9d-8dcb-6c10f49369e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“¦ Extracting archives...\n",
            "âœ… Archives extracted successfully\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nğŸ“¦ Extracting archives...\")\n",
        "\n",
        "!unzip -q ./Train.zip\n",
        "!unzip -q ./Train_annotation.zip\n",
        "!unzip -q ./Test.zip\n",
        "\n",
        "print(\"âœ… Archives extracted successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f659e08b",
      "metadata": {
        "id": "f659e08b",
        "papermill": {
          "duration": 0.028263,
          "end_time": "2025-11-17T07:55:04.767999",
          "exception": false,
          "start_time": "2025-11-17T07:55:04.739736",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Clean Up Zip Files (Only for Kaggle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6edac85",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:55:04.826893Z",
          "iopub.status.busy": "2025-11-17T07:55:04.826234Z",
          "iopub.status.idle": "2025-11-17T07:55:04.909122Z",
          "shell.execute_reply": "2025-11-17T07:55:04.908391Z"
        },
        "id": "e6edac85",
        "papermill": {
          "duration": 0.113582,
          "end_time": "2025-11-17T07:55:04.910348",
          "exception": false,
          "start_time": "2025-11-17T07:55:04.796766",
          "status": "completed"
        },
        "tags": [],
        "outputId": "72c1e5b1-e2fd-4877-e974-d33a105ba595"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ—‘ï¸ Cleaning up zip files...\n",
            "âœ… Cleanup complete\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nğŸ—‘ï¸ Cleaning up zip files...\")\n",
        "\n",
        "os.remove(\"/kaggle/working/Test.zip\")\n",
        "os.remove(\"/kaggle/working/Train.zip\")\n",
        "os.remove(\"/kaggle/working/Train_annotation.zip\")\n",
        "\n",
        "print(\"âœ… Cleanup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85197cec",
      "metadata": {
        "id": "85197cec",
        "papermill": {
          "duration": 0.028359,
          "end_time": "2025-11-17T07:55:04.967775",
          "exception": false,
          "start_time": "2025-11-17T07:55:04.939416",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Define Dataset Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bb9b6ea",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:55:05.025493Z",
          "iopub.status.busy": "2025-11-17T07:55:05.025172Z",
          "iopub.status.idle": "2025-11-17T07:55:05.030204Z",
          "shell.execute_reply": "2025-11-17T07:55:05.029397Z"
        },
        "id": "9bb9b6ea",
        "papermill": {
          "duration": 0.035031,
          "end_time": "2025-11-17T07:55:05.031191",
          "exception": false,
          "start_time": "2025-11-17T07:55:04.996160",
          "status": "completed"
        },
        "tags": [],
        "outputId": "0a2e03f3-d2f0-4719-f2bd-fac5f3d1ba44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“ Configured for 20 regional dialects\n",
            "   Audio path: ./Train/\n",
            "   Annotation path: ./Train_annotation/\n"
          ]
        }
      ],
      "source": [
        "# Dataset paths\n",
        "AUDIO_BASE_PATH = \"./Train/\"\n",
        "ANNOTATION_BASE_PATH = \"./Train_annotation/\"\n",
        "\n",
        "# List of all 20 Bengali regional dialects\n",
        "REGIONS = [\n",
        "    \"Barisal\", \"Bhola\", \"Bogura\", \"Brahmanbaria\", \"Chittagong\", \"Comilla\",\n",
        "    \"Dhaka\", \"Feni\", \"Jessore\", \"Jhenaidah\", \"Khulna\", \"Kushtia\",\n",
        "    \"Lakshmipur\", \"Mymensingh\", \"Natore\", \"Noakhali\", \"Pabna\", \"Rajshahi\",\n",
        "    \"Rangpur\", \"Sylhet\"\n",
        "]\n",
        "\n",
        "print(f\"\\nğŸ“ Configured for {len(REGIONS)} regional dialects\")\n",
        "print(f\"   Audio path: {AUDIO_BASE_PATH}\")\n",
        "print(f\"   Annotation path: {ANNOTATION_BASE_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66149c79",
      "metadata": {
        "id": "66149c79",
        "papermill": {
          "duration": 0.028656,
          "end_time": "2025-11-17T07:55:05.088276",
          "exception": false,
          "start_time": "2025-11-17T07:55:05.059620",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# IMPORT REQUIRED LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2310a61",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:55:05.148859Z",
          "iopub.status.busy": "2025-11-17T07:55:05.148384Z",
          "iopub.status.idle": "2025-11-17T07:55:29.783827Z",
          "shell.execute_reply": "2025-11-17T07:55:29.782987Z"
        },
        "id": "b2310a61",
        "papermill": {
          "duration": 24.667529,
          "end_time": "2025-11-17T07:55:29.785111",
          "exception": false,
          "start_time": "2025-11-17T07:55:05.117582",
          "status": "completed"
        },
        "tags": [],
        "outputId": "c0f1cc5d-cf1e-4e3a-90a7-2cebd64f8675"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Data processing libraries imported\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-17 07:55:15.043495: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763366115.216776      20 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763366115.267601      20 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Transformers components imported\n",
            "âœ… Training utilities imported\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset, DatasetDict, Audio\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"\\nâœ… Data processing libraries imported\")\n",
        "\n",
        "from transformers import (\n",
        "    WhisperFeatureExtractor,\n",
        "    WhisperTokenizer,\n",
        "    WhisperProcessor,\n",
        "    WhisperForConditionalGeneration,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer\n",
        ")\n",
        "\n",
        "print(\"âœ… Transformers components imported\")\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "import evaluate\n",
        "\n",
        "print(\"âœ… Training utilities imported\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ee93056",
      "metadata": {
        "id": "3ee93056",
        "papermill": {
          "duration": 0.029592,
          "end_time": "2025-11-17T07:55:29.844957",
          "exception": false,
          "start_time": "2025-11-17T07:55:29.815365",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# DATASET LOADING"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02cafe63",
      "metadata": {
        "id": "02cafe63",
        "papermill": {
          "duration": 0.029622,
          "end_time": "2025-11-17T07:55:29.903880",
          "exception": false,
          "start_time": "2025-11-17T07:55:29.874258",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Define Dataset Loading Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "388917e7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:55:29.965499Z",
          "iopub.status.busy": "2025-11-17T07:55:29.964101Z",
          "iopub.status.idle": "2025-11-17T07:55:29.975959Z",
          "shell.execute_reply": "2025-11-17T07:55:29.975332Z"
        },
        "id": "388917e7",
        "papermill": {
          "duration": 0.04361,
          "end_time": "2025-11-17T07:55:29.977137",
          "exception": false,
          "start_time": "2025-11-17T07:55:29.933527",
          "status": "completed"
        },
        "tags": [],
        "outputId": "1a594530-7209-4a37-fa5b-0a1728644732"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Dataset loading function defined\n"
          ]
        }
      ],
      "source": [
        "def load_custom_dataset(audio_base_path, annotation_base_path, regions, split_ratio=0.9):\n",
        "    \"\"\"\n",
        "    Load custom Bengali regional ASR dataset from audio files and CSV annotations\n",
        "\n",
        "    Args:\n",
        "        audio_base_path: Base path to audio files\n",
        "        annotation_base_path: Base path to annotation CSV files\n",
        "        regions: List of region names\n",
        "        split_ratio: Train/test split ratio (default: 0.9)\n",
        "\n",
        "    Returns:\n",
        "        DatasetDict with 'train' and 'test' splits\n",
        "    \"\"\"\n",
        "    all_data = []\n",
        "\n",
        "    for region in regions:\n",
        "        csv_path = os.path.join(annotation_base_path, f\"{region}.csv\")\n",
        "        audio_folder = os.path.join(audio_base_path, region)\n",
        "\n",
        "        # Check if files exist\n",
        "        if not os.path.exists(csv_path):\n",
        "            print(f\"âš ï¸ CSV file not found for {region}\")\n",
        "            continue\n",
        "\n",
        "        if not os.path.exists(audio_folder):\n",
        "            print(f\"âš ï¸ Audio folder not found for {region}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(csv_path)\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error reading CSV for {region}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Process each row in the CSV\n",
        "        for idx, row in df.iterrows():\n",
        "            try:\n",
        "                audio_filename = str(row.iloc[0]).strip()\n",
        "                transcription = str(row.iloc[1]).strip()\n",
        "\n",
        "                # Skip invalid transcriptions\n",
        "                if not transcription or transcription == 'nan' or pd.isna(transcription):\n",
        "                    continue\n",
        "\n",
        "                audio_path = os.path.join(audio_folder, audio_filename)\n",
        "\n",
        "                # Check if audio file exists and is not empty\n",
        "                if os.path.exists(audio_path) and os.path.getsize(audio_path) > 0:\n",
        "                    all_data.append({\n",
        "                        \"path\": audio_path,\n",
        "                        \"sentence\": transcription\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        # Clean up DataFrame to free memory\n",
        "        del df\n",
        "        gc.collect()\n",
        "\n",
        "    print(f\"ğŸ“Š Total samples loaded: {len(all_data)}\")\n",
        "\n",
        "    if len(all_data) == 0:\n",
        "        raise ValueError(\"âŒ No valid samples found. Please check your dataset paths and files.\")\n",
        "\n",
        "    # Split into train and test\n",
        "    split_idx = int(len(all_data) * split_ratio)\n",
        "    if split_idx == 0:\n",
        "        split_idx = 1\n",
        "    if split_idx >= len(all_data):\n",
        "        split_idx = len(all_data) - 1\n",
        "\n",
        "    train_data = all_data[:split_idx]\n",
        "    test_data = all_data[split_idx:]\n",
        "\n",
        "    # Create datasets\n",
        "    train_dict = {\n",
        "        \"path\": [item[\"path\"] for item in train_data],\n",
        "        \"sentence\": [item[\"sentence\"] for item in train_data]\n",
        "    }\n",
        "\n",
        "    test_dict = {\n",
        "        \"path\": [item[\"path\"] for item in test_data],\n",
        "        \"sentence\": [item[\"sentence\"] for item in test_data]\n",
        "    }\n",
        "\n",
        "    train_dataset = Dataset.from_dict(train_dict)\n",
        "    test_dataset = Dataset.from_dict(test_dict)\n",
        "\n",
        "    # Clear temporary data\n",
        "    del all_data, train_data, test_data, train_dict, test_dict\n",
        "    gc.collect()\n",
        "\n",
        "    # Cast to audio format\n",
        "    train_dataset = train_dataset.cast_column(\"path\", Audio(sampling_rate=16000))\n",
        "    test_dataset = test_dataset.cast_column(\"path\", Audio(sampling_rate=16000))\n",
        "\n",
        "    # Rename column\n",
        "    train_dataset = train_dataset.rename_column(\"path\", \"audio\")\n",
        "    test_dataset = test_dataset.rename_column(\"path\", \"audio\")\n",
        "\n",
        "    return DatasetDict({\n",
        "        \"train\": train_dataset,\n",
        "        \"test\": test_dataset\n",
        "    })\n",
        "\n",
        "print(\"âœ… Dataset loading function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4986a51",
      "metadata": {
        "id": "d4986a51",
        "papermill": {
          "duration": 0.029509,
          "end_time": "2025-11-17T07:55:30.036500",
          "exception": false,
          "start_time": "2025-11-17T07:55:30.006991",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "667dda5a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:55:30.096588Z",
          "iopub.status.busy": "2025-11-17T07:55:30.096298Z",
          "iopub.status.idle": "2025-11-17T07:55:38.188010Z",
          "shell.execute_reply": "2025-11-17T07:55:38.187257Z"
        },
        "id": "667dda5a",
        "papermill": {
          "duration": 8.123844,
          "end_time": "2025-11-17T07:55:38.189294",
          "exception": false,
          "start_time": "2025-11-17T07:55:30.065450",
          "status": "completed"
        },
        "tags": [],
        "outputId": "32dacbbb-9a73-47a6-ab3c-bd923a4786c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "LOADING DATASET\n",
            "======================================================================\n",
            "\n",
            "ğŸ“Š Total samples loaded: 3350\n",
            "\n",
            "ğŸ“Š Dataset Structure:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['audio', 'sentence'],\n",
            "        num_rows: 3015\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['audio', 'sentence'],\n",
            "        num_rows: 335\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"LOADING DATASET\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "custom_voice = load_custom_dataset(AUDIO_BASE_PATH, ANNOTATION_BASE_PATH, REGIONS)\n",
        "\n",
        "print(\"\\nğŸ“Š Dataset Structure:\")\n",
        "print(custom_voice)\n",
        "\n",
        "# Memory cleanup\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4537951",
      "metadata": {
        "id": "f4537951",
        "papermill": {
          "duration": 0.029516,
          "end_time": "2025-11-17T07:55:38.249631",
          "exception": false,
          "start_time": "2025-11-17T07:55:38.220115",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# MODEL AND PROCESSOR SETUP\n",
        "## We utilized [`bengaliAI/tugstugi_bengaliai-regional-asr_whisper-medium`](https://huggingface.co/bengaliAI/tugstugi_bengaliai-regional-asr_whisper-medium) model which originally [`openai/whisper-medium`](https://huggingface.co/openai/whisper-medium) model, finetuned in [`Ben10`](https://www.kaggle.com/competitions/ben10) Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96ac322d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:55:38.311840Z",
          "iopub.status.busy": "2025-11-17T07:55:38.311556Z",
          "iopub.status.idle": "2025-11-17T07:55:39.985541Z",
          "shell.execute_reply": "2025-11-17T07:55:39.984628Z"
        },
        "id": "96ac322d",
        "papermill": {
          "duration": 1.706256,
          "end_time": "2025-11-17T07:55:39.986869",
          "exception": false,
          "start_time": "2025-11-17T07:55:38.280613",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "d89fca9c2ffb40c19ba9c206e2333139",
            "decc87dd02bc484aa196554420a3175d",
            "192e96bcf84b4207abfc18f284d302b5",
            "a9fc40e738874bfb8099f7466707b6b0",
            "13304eafd35f4f41bc23804e3eea5a65",
            "e8c8f42badcd4618a692ee4ad14fd1dc",
            "3e25916ad0d04566a220c520890da025",
            "199599111012422f942c88db1b6c59c3"
          ]
        },
        "outputId": "44001aa9-8d13-4d6b-bf5b-b490f97d1513"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "LOADING WHISPER PROCESSOR\n",
            "======================================================================\n",
            "\n",
            "ğŸ“¦ Loading from: bengaliAI/tugstugi_bengaliai-regional-asr_whisper-medium\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d89fca9c2ffb40c19ba9c206e2333139",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/339 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "decc87dd02bc484aa196554420a3175d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "192e96bcf84b4207abfc18f284d302b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9fc40e738874bfb8099f7466707b6b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13304eafd35f4f41bc23804e3eea5a65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8c8f42badcd4618a692ee4ad14fd1dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "normalizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e25916ad0d04566a220c520890da025",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "199599111012422f942c88db1b6c59c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processor loaded successfully\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"LOADING WHISPER PROCESSOR\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "model_name = \"bengaliAI/tugstugi_bengaliai-regional-asr_whisper-medium\"\n",
        "\n",
        "print(f\"ğŸ“¦ Loading from: {model_name}\")\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    language=\"Bengali\",\n",
        "    task=\"transcribe\"\n",
        ")\n",
        "processor = WhisperProcessor.from_pretrained(\n",
        "    model_name,\n",
        "    language=\"Bengali\",\n",
        "    task=\"transcribe\"\n",
        ")\n",
        "\n",
        "print(\"âœ… Processor loaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4553f5f9",
      "metadata": {
        "id": "4553f5f9",
        "papermill": {
          "duration": 0.03022,
          "end_time": "2025-11-17T07:55:40.048126",
          "exception": false,
          "start_time": "2025-11-17T07:55:40.017906",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# DATA PREPROCESSING"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73f08bc5",
      "metadata": {
        "id": "73f08bc5",
        "papermill": {
          "duration": 0.030239,
          "end_time": "2025-11-17T07:55:40.108621",
          "exception": false,
          "start_time": "2025-11-17T07:55:40.078382",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Define Preprocessing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69eaea11",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:55:40.170277Z",
          "iopub.status.busy": "2025-11-17T07:55:40.169730Z",
          "iopub.status.idle": "2025-11-17T07:55:40.174727Z",
          "shell.execute_reply": "2025-11-17T07:55:40.174060Z"
        },
        "id": "69eaea11",
        "papermill": {
          "duration": 0.037301,
          "end_time": "2025-11-17T07:55:40.175763",
          "exception": false,
          "start_time": "2025-11-17T07:55:40.138462",
          "status": "completed"
        },
        "tags": [],
        "outputId": "deb4d8b2-991b-44b2-fbc5-d0a8ab2351f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Preprocessing function defined\n"
          ]
        }
      ],
      "source": [
        "def prepare_dataset(batch):\n",
        "    \"\"\"\n",
        "    Prepare dataset batch for training\n",
        "\n",
        "    - Converts audio to log-Mel spectrogram features\n",
        "    - Tokenizes transcriptions to label IDs\n",
        "\n",
        "    Args:\n",
        "        batch: Single batch from dataset\n",
        "\n",
        "    Returns:\n",
        "        Processed batch with input_features and labels\n",
        "    \"\"\"\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # Compute log-Mel input features from input audio array\n",
        "    batch[\"input_features\"] = feature_extractor(\n",
        "        audio[\"array\"],\n",
        "        sampling_rate=audio[\"sampling_rate\"]\n",
        "    ).input_features[0]\n",
        "\n",
        "    # Encode target text to label ids\n",
        "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "\n",
        "    return batch\n",
        "\n",
        "print(\"âœ… Preprocessing function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f660dcf9",
      "metadata": {
        "id": "f660dcf9",
        "papermill": {
          "duration": 0.03163,
          "end_time": "2025-11-17T07:55:40.324999",
          "exception": false,
          "start_time": "2025-11-17T07:55:40.293369",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Preprocess the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1328e6de",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:55:40.386520Z",
          "iopub.status.busy": "2025-11-17T07:55:40.386234Z",
          "iopub.status.idle": "2025-11-17T07:56:40.986085Z",
          "shell.execute_reply": "2025-11-17T07:56:40.985263Z"
        },
        "id": "1328e6de",
        "papermill": {
          "duration": 60.639263,
          "end_time": "2025-11-17T07:56:40.994413",
          "exception": false,
          "start_time": "2025-11-17T07:55:40.355150",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "c5e976e83cbb4ba28b6234984cec429d",
            "8caf2f119e454056abd4bd8fb0f13846"
          ]
        },
        "outputId": "b5c630d1-5bb1-4d74-a9a4-acb6e7253cf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "PREPROCESSING DATASET\n",
            "======================================================================\n",
            "\n",
            "â³ Processing dataset (this may take a while)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5e976e83cbb4ba28b6234984cec429d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Preparing dataset:   0%|          | 0/3015 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8caf2f119e454056abd4bd8fb0f13846",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Preparing dataset:   0%|          | 0/335 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Dataset preprocessing complete\n",
            "\n",
            "ğŸ“Š Processed Dataset Structure:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_features', 'labels'],\n",
            "        num_rows: 3015\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_features', 'labels'],\n",
            "        num_rows: 335\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PREPROCESSING DATASET\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "print(\"â³ Processing dataset (this may take a while)...\")\n",
        "\n",
        "custom_voice = custom_voice.map(\n",
        "    prepare_dataset,\n",
        "    remove_columns=custom_voice.column_names[\"train\"],\n",
        "    desc=\"Preparing dataset\",\n",
        "    batched=False,\n",
        "    num_proc=1\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Dataset preprocessing complete\")\n",
        "print(f\"\\nğŸ“Š Processed Dataset Structure:\")\n",
        "print(custom_voice)\n",
        "\n",
        "# Memory cleanup\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "816a889c",
      "metadata": {
        "id": "816a889c",
        "papermill": {
          "duration": 0.031016,
          "end_time": "2025-11-17T07:56:41.056268",
          "exception": false,
          "start_time": "2025-11-17T07:56:41.025252",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# DATA COLLATOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5d76078",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:56:41.118522Z",
          "iopub.status.busy": "2025-11-17T07:56:41.117704Z",
          "iopub.status.idle": "2025-11-17T07:56:41.124842Z",
          "shell.execute_reply": "2025-11-17T07:56:41.124268Z"
        },
        "id": "e5d76078",
        "papermill": {
          "duration": 0.039265,
          "end_time": "2025-11-17T07:56:41.125836",
          "exception": false,
          "start_time": "2025-11-17T07:56:41.086571",
          "status": "completed"
        },
        "tags": [],
        "outputId": "685f4d97-c4c3-4422-8970-71b6dd56cb9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Data collator class defined\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    \"\"\"\n",
        "    Data collator for speech-to-text models\n",
        "\n",
        "    - Pads input features to same length\n",
        "    - Pads labels and masks padding tokens with -100\n",
        "    - Removes BOS token from labels if present\n",
        "    \"\"\"\n",
        "    processor: Any\n",
        "    decoder_start_token_id: int\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # Split inputs and labels\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # Get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # Replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # If bos token is appended in previous tokenization step, cut bos token here\n",
        "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "print(\"âœ… Data collator class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80838c89",
      "metadata": {
        "id": "80838c89",
        "papermill": {
          "duration": 0.029773,
          "end_time": "2025-11-17T07:56:41.186035",
          "exception": false,
          "start_time": "2025-11-17T07:56:41.156262",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# MODEL LOADING"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe1928cd",
      "metadata": {
        "id": "fe1928cd",
        "papermill": {
          "duration": 0.030161,
          "end_time": "2025-11-17T07:56:41.246850",
          "exception": false,
          "start_time": "2025-11-17T07:56:41.216689",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Pre-Load Memory Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ea3f8d3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:56:41.310925Z",
          "iopub.status.busy": "2025-11-17T07:56:41.310368Z",
          "iopub.status.idle": "2025-11-17T07:56:41.702339Z",
          "shell.execute_reply": "2025-11-17T07:56:41.701475Z"
        },
        "id": "7ea3f8d3",
        "papermill": {
          "duration": 0.426712,
          "end_time": "2025-11-17T07:56:41.703521",
          "exception": false,
          "start_time": "2025-11-17T07:56:41.276809",
          "status": "completed"
        },
        "tags": [],
        "outputId": "3a91dffc-f80a-4c72-e9fb-a0929cc0dbb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "LOADING WHISPER MODEL\n",
            "======================================================================\n",
            "\n",
            "ğŸ§¹ Clearing GPU memory before model load...\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"LOADING WHISPER MODEL\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "print(\"ğŸ§¹ Clearing GPU memory before model load...\")\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cdeb01e",
      "metadata": {
        "id": "1cdeb01e",
        "papermill": {
          "duration": 0.031362,
          "end_time": "2025-11-17T07:56:41.766775",
          "exception": false,
          "start_time": "2025-11-17T07:56:41.735413",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Load Pre-trained Whisper Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bf0fa8a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:56:41.829357Z",
          "iopub.status.busy": "2025-11-17T07:56:41.828843Z",
          "iopub.status.idle": "2025-11-17T07:56:54.131594Z",
          "shell.execute_reply": "2025-11-17T07:56:54.130801Z"
        },
        "id": "1bf0fa8a",
        "papermill": {
          "duration": 12.336489,
          "end_time": "2025-11-17T07:56:54.133964",
          "exception": false,
          "start_time": "2025-11-17T07:56:41.797475",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "80d0b4f1401f4d58a85f6c7d46fcc515",
            "dcd7b023a7c647ca8926eff98eb4daf3",
            "7977630a5fbf4074b5da087236f0fddd"
          ]
        },
        "outputId": "c9404c45-fc20-4f56-ef85-ba514e4a7588"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“¦ Loading model: bengaliAI/tugstugi_bengaliai-regional-asr_whisper-medium\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80d0b4f1401f4d58a85f6c7d46fcc515",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dcd7b023a7c647ca8926eff98eb4daf3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.06G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7977630a5fbf4074b5da087236f0fddd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/223 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(f\"ğŸ“¦ Loading model: {model_name}\")\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\n",
        "    model_name,\n",
        "    use_cache=True,\n",
        "    low_cpu_mem_usage=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4eba3e8",
      "metadata": {
        "id": "d4eba3e8",
        "papermill": {
          "duration": 0.067398,
          "end_time": "2025-11-17T07:56:54.259824",
          "exception": false,
          "start_time": "2025-11-17T07:56:54.192426",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Fix Language/Task Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c32f6305",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:56:54.354158Z",
          "iopub.status.busy": "2025-11-17T07:56:54.353458Z",
          "iopub.status.idle": "2025-11-17T07:56:54.360419Z",
          "shell.execute_reply": "2025-11-17T07:56:54.359465Z"
        },
        "id": "c32f6305",
        "papermill": {
          "duration": 0.058012,
          "end_time": "2025-11-17T07:56:54.361641",
          "exception": false,
          "start_time": "2025-11-17T07:56:54.303629",
          "status": "completed"
        },
        "tags": [],
        "outputId": "2e2b6710-5912-4f61-df10-b5ec7dbce1d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ”§ Fixing generation config...\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nğŸ”§ Fixing generation config...\")\n",
        "\n",
        "# Remove language/task attributes that cause errors\n",
        "if hasattr(model.generation_config, 'language'):\n",
        "    delattr(model.generation_config, 'language')\n",
        "if hasattr(model.generation_config, 'task'):\n",
        "    delattr(model.generation_config, 'task')\n",
        "\n",
        "# Ensure forced_decoder_ids stays None\n",
        "model.generation_config.forced_decoder_ids = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f772665",
      "metadata": {
        "id": "6f772665",
        "papermill": {
          "duration": 0.041792,
          "end_time": "2025-11-17T07:56:54.455644",
          "exception": false,
          "start_time": "2025-11-17T07:56:54.413852",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Verify Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c493f972",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:56:54.546490Z",
          "iopub.status.busy": "2025-11-17T07:56:54.546075Z",
          "iopub.status.idle": "2025-11-17T07:56:54.553442Z",
          "shell.execute_reply": "2025-11-17T07:56:54.552496Z"
        },
        "id": "c493f972",
        "papermill": {
          "duration": 0.054127,
          "end_time": "2025-11-17T07:56:54.555379",
          "exception": false,
          "start_time": "2025-11-17T07:56:54.501252",
          "status": "completed"
        },
        "tags": [],
        "outputId": "88a277b6-eb9e-4cee-a38b-7541d5b7748b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Model loaded successfully\n",
            "ğŸ’¾ GPU Memory after model load: 0.00 GB\n",
            "\n",
            "ğŸ” Generation Config Verification:\n",
            "   forced_decoder_ids: None\n",
            "   Has language attr: False\n",
            "   Has task attr: False\n",
            "   Has lang_to_id: False\n",
            "âœ… Config is clean - no language forcing will occur during generation\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nâœ… Model loaded successfully\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"ğŸ’¾ GPU Memory after model load: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
        "\n",
        "print(f\"\\nğŸ” Generation Config Verification:\")\n",
        "print(f\"   forced_decoder_ids: {model.generation_config.forced_decoder_ids}\")\n",
        "print(f\"   Has language attr: {hasattr(model.generation_config, 'language')}\")\n",
        "print(f\"   Has task attr: {hasattr(model.generation_config, 'task')}\")\n",
        "print(f\"   Has lang_to_id: {hasattr(model.generation_config, 'lang_to_id')}\")\n",
        "print(\"âœ… Config is clean - no language forcing will occur during generation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f53b5ce",
      "metadata": {
        "id": "6f53b5ce",
        "papermill": {
          "duration": 0.036571,
          "end_time": "2025-11-17T07:56:54.639294",
          "exception": false,
          "start_time": "2025-11-17T07:56:54.602723",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Initialize Data Collator Instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bda3061",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:56:54.732331Z",
          "iopub.status.busy": "2025-11-17T07:56:54.731839Z",
          "iopub.status.idle": "2025-11-17T07:56:54.737351Z",
          "shell.execute_reply": "2025-11-17T07:56:54.736466Z"
        },
        "id": "5bda3061",
        "papermill": {
          "duration": 0.059177,
          "end_time": "2025-11-17T07:56:54.739251",
          "exception": false,
          "start_time": "2025-11-17T07:56:54.680074",
          "status": "completed"
        },
        "tags": [],
        "outputId": "49f95d75-115f-48bf-8046-e917580bc98c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Data collator initialized\n"
          ]
        }
      ],
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
        "    processor=processor,\n",
        "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Data collator initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d3888cd",
      "metadata": {
        "id": "6d3888cd",
        "papermill": {
          "duration": 0.043316,
          "end_time": "2025-11-17T07:56:54.839719",
          "exception": false,
          "start_time": "2025-11-17T07:56:54.796403",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# EVALUATION METRICS"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2329bac",
      "metadata": {
        "id": "f2329bac",
        "papermill": {
          "duration": 0.0541,
          "end_time": "2025-11-17T07:56:54.948292",
          "exception": false,
          "start_time": "2025-11-17T07:56:54.894192",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Load WER Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c0fe7a1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:56:55.049718Z",
          "iopub.status.busy": "2025-11-17T07:56:55.049400Z",
          "iopub.status.idle": "2025-11-17T07:56:55.878347Z",
          "shell.execute_reply": "2025-11-17T07:56:55.877618Z"
        },
        "id": "5c0fe7a1",
        "papermill": {
          "duration": 0.884058,
          "end_time": "2025-11-17T07:56:55.879518",
          "exception": false,
          "start_time": "2025-11-17T07:56:54.995460",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "6f1ebd1b04e7415385abc2f698e99ba6"
          ]
        },
        "outputId": "43ceab51-a5b1-4378-8249-553da759a5e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "LOADING EVALUATION METRICS\n",
            "======================================================================\n",
            "\n",
            "ğŸ“Š Loading WER (Word Error Rate) metric...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f1ebd1b04e7415385abc2f698e99ba6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… WER metric loaded from evaluate\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"LOADING EVALUATION METRICS\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "print(\"ğŸ“Š Loading WER (Word Error Rate) metric...\")\n",
        "\n",
        "try:\n",
        "    metric = evaluate.load(\"wer\", trust_remote_code=True)\n",
        "    print(\"âœ… WER metric loaded from evaluate\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Could not load from evaluate: {e}\")\n",
        "    print(\"ğŸ”„ Using jiwer as fallback...\")\n",
        "\n",
        "    import jiwer\n",
        "\n",
        "    class WERMetric:\n",
        "        \"\"\"Wrapper for jiwer to match evaluate interface\"\"\"\n",
        "        def compute(self, predictions, references):\n",
        "            if isinstance(predictions, str):\n",
        "                predictions = [predictions]\n",
        "            if isinstance(references, str):\n",
        "                references = [references]\n",
        "            return jiwer.wer(references, predictions)\n",
        "\n",
        "    metric = WERMetric()\n",
        "    print(\"âœ… Using jiwer for WER calculation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7474305",
      "metadata": {
        "id": "c7474305",
        "papermill": {
          "duration": 0.033185,
          "end_time": "2025-11-17T07:56:55.946484",
          "exception": false,
          "start_time": "2025-11-17T07:56:55.913299",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Define Compute Metrics Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c1ac982",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:56:56.013182Z",
          "iopub.status.busy": "2025-11-17T07:56:56.012910Z",
          "iopub.status.idle": "2025-11-17T07:56:56.018104Z",
          "shell.execute_reply": "2025-11-17T07:56:56.017499Z"
        },
        "id": "3c1ac982",
        "papermill": {
          "duration": 0.039708,
          "end_time": "2025-11-17T07:56:56.019161",
          "exception": false,
          "start_time": "2025-11-17T07:56:55.979453",
          "status": "completed"
        },
        "tags": [],
        "outputId": "9243666b-e311-4234-8515-6031f4de9fec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Compute metrics function defined\n"
          ]
        }
      ],
      "source": [
        "def compute_metrics(pred):\n",
        "    \"\"\"\n",
        "    Compute Word Error Rate (WER) for model predictions\n",
        "\n",
        "    Args:\n",
        "        pred: Predictions object with predictions and labels\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with WER score (lower is better)\n",
        "    \"\"\"\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # Replace -100 with pad_token_id\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # Decode predictions and labels\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Compute WER\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}\n",
        "\n",
        "print(\"âœ… Compute metrics function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11435cea",
      "metadata": {
        "id": "11435cea",
        "papermill": {
          "duration": 0.035023,
          "end_time": "2025-11-17T07:56:56.086137",
          "exception": false,
          "start_time": "2025-11-17T07:56:56.051114",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# TRAINING CONFIGURATION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fb22ac0",
      "metadata": {
        "id": "5fb22ac0",
        "papermill": {
          "duration": 0.03121,
          "end_time": "2025-11-17T07:56:56.148785",
          "exception": false,
          "start_time": "2025-11-17T07:56:56.117575",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Define Training Arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ca34afa",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:56:56.214204Z",
          "iopub.status.busy": "2025-11-17T07:56:56.213751Z",
          "iopub.status.idle": "2025-11-17T07:56:56.257985Z",
          "shell.execute_reply": "2025-11-17T07:56:56.257261Z"
        },
        "id": "2ca34afa",
        "papermill": {
          "duration": 0.077803,
          "end_time": "2025-11-17T07:56:56.259111",
          "exception": false,
          "start_time": "2025-11-17T07:56:56.181308",
          "status": "completed"
        },
        "tags": [],
        "outputId": "0921d1fe-8b95-4a5e-b1ec-4525600c5655"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "CONFIGURING TRAINING\n",
            "======================================================================\n",
            "\n",
            "âœ… Training arguments configured\n",
            "\n",
            "ğŸ“‹ Training Configuration Summary:\n",
            "   Output Directory: ./whisper-medium-bengali-regional\n",
            "   Batch Size: 4\n",
            "   Gradient Accumulation: 4\n",
            "   Effective Batch Size: 16\n",
            "   Learning Rate: 1e-05\n",
            "   Max Steps: 4000\n",
            "   FP16: True\n",
            "   Gradient Checkpointing: True\n",
            "   Save Total Limit: 1 (only latest checkpoint)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CONFIGURING TRAINING\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-medium-bengali-regional\",\n",
        "\n",
        "    # ========== Batch Configuration ==========\n",
        "    per_device_train_batch_size=4,        # Batch size per GPU\n",
        "    gradient_accumulation_steps=4,         # Accumulate gradients\n",
        "    per_device_eval_batch_size=4,         # Eval batch size\n",
        "\n",
        "    # ========== Learning Rate ==========\n",
        "    learning_rate=1e-5,                    # Conservative learning rate\n",
        "    warmup_steps=200,                      # Warmup for stable training\n",
        "\n",
        "    # ========== Training Duration ==========\n",
        "    max_steps=4000,                        # Total training steps (~3.5-4 hours)\n",
        "\n",
        "    # ========== Memory Optimization ==========\n",
        "    gradient_checkpointing=True,           # Save memory with checkpointing\n",
        "    fp16=True,                             # Use mixed precision\n",
        "    fp16_opt_level=\"O1\",                   # FP16 optimization level\n",
        "\n",
        "    # ========== Evaluation Settings ==========\n",
        "    eval_strategy=\"steps\",                 # Evaluate at fixed steps\n",
        "    predict_with_generate=True,            # Generate predictions for WER\n",
        "    generation_max_length=225,             # Max generation length\n",
        "\n",
        "    # ========== Checkpoint Management ==========\n",
        "    save_steps=1000,                       # Save every 1000 steps\n",
        "    eval_steps=1000,                       # Evaluate every 1000 steps\n",
        "    logging_steps=50,                      # Log every 50 steps\n",
        "    save_total_limit=1,                    # Keep only latest checkpoint\n",
        "    load_best_model_at_end=True,           # Load best model after training\n",
        "    metric_for_best_model=\"wer\",           # Use WER for best model\n",
        "    greater_is_better=False,               # Lower WER is better\n",
        "\n",
        "    # ========== Data Loading ==========\n",
        "    dataloader_num_workers=2,              # Parallel data loading\n",
        "    dataloader_pin_memory=True,            # Pin memory for faster transfer\n",
        "\n",
        "    # ========== Optimizer ==========\n",
        "    optim=\"adamw_torch\",                   # AdamW optimizer\n",
        "    max_grad_norm=1.0,                     # Gradient clipping\n",
        "\n",
        "    # ========== Logging and Output ==========\n",
        "    report_to=[\"tensorboard\"],             # Use TensorBoard for monitoring\n",
        "    push_to_hub=False,                     # Don't push to Hugging Face Hub\n",
        "\n",
        "    # ========== GPU Configuration ==========\n",
        "    no_cuda=False,                         # Use CUDA\n",
        "    local_rank=-1,                         # Disable distributed training\n",
        ")\n",
        "\n",
        "print(\"âœ… Training arguments configured\")\n",
        "\n",
        "print(\"\\nğŸ“‹ Training Configuration Summary:\")\n",
        "print(f\"   Output Directory: {training_args.output_dir}\")\n",
        "print(f\"   Batch Size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"   Gradient Accumulation: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"   Effective Batch Size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"   Learning Rate: {training_args.learning_rate}\")\n",
        "print(f\"   Max Steps: {training_args.max_steps}\")\n",
        "print(f\"   FP16: {training_args.fp16}\")\n",
        "print(f\"   Gradient Checkpointing: {training_args.gradient_checkpointing}\")\n",
        "print(f\"   Save Total Limit: {training_args.save_total_limit} (only latest checkpoint)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a099311a",
      "metadata": {
        "id": "a099311a",
        "papermill": {
          "duration": 0.031739,
          "end_time": "2025-11-17T07:56:56.325894",
          "exception": false,
          "start_time": "2025-11-17T07:56:56.294155",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Verify Single GPU Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b8c1e29",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:56:56.391426Z",
          "iopub.status.busy": "2025-11-17T07:56:56.390864Z",
          "iopub.status.idle": "2025-11-17T07:56:56.836869Z",
          "shell.execute_reply": "2025-11-17T07:56:56.836109Z"
        },
        "id": "3b8c1e29",
        "papermill": {
          "duration": 0.479444,
          "end_time": "2025-11-17T07:56:56.837879",
          "exception": false,
          "start_time": "2025-11-17T07:56:56.358435",
          "status": "completed"
        },
        "tags": [],
        "outputId": "d31408a2-1412-4632-9573-ee1db8a9de9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ” Verifying GPU Configuration:\n",
            "   CUDA_VISIBLE_DEVICES: 0\n",
            "   Number of GPUs PyTorch sees: 1\n",
            "   Current device: 0\n",
            "   GPU Memory allocated: 0.00 GB\n",
            "   GPU Memory reserved: 0.00 GB\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nğŸ” Verifying GPU Configuration:\")\n",
        "print(f\"   CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set')}\")\n",
        "print(f\"   Number of GPUs PyTorch sees: {torch.cuda.device_count()}\")\n",
        "print(f\"   Current device: {torch.cuda.current_device()}\")\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"   GPU Memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
        "    print(f\"   GPU Memory reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6fe3fbd",
      "metadata": {
        "id": "b6fe3fbd",
        "papermill": {
          "duration": 0.032864,
          "end_time": "2025-11-17T07:56:56.903280",
          "exception": false,
          "start_time": "2025-11-17T07:56:56.870416",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# CUSTOM CALLBACKS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3022273",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:56:56.969374Z",
          "iopub.status.busy": "2025-11-17T07:56:56.969065Z",
          "iopub.status.idle": "2025-11-17T07:56:56.977724Z",
          "shell.execute_reply": "2025-11-17T07:56:56.976962Z"
        },
        "id": "b3022273",
        "papermill": {
          "duration": 0.042573,
          "end_time": "2025-11-17T07:56:56.978801",
          "exception": false,
          "start_time": "2025-11-17T07:56:56.936228",
          "status": "completed"
        },
        "tags": [],
        "outputId": "394e38fe-2df2-40e4-bab8-473a1fa93284"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Memory cleanup callback created with checkpoint management\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainerCallback\n",
        "\n",
        "class MemoryCleanupCallback(TrainerCallback):\n",
        "    \"\"\"\n",
        "    Custom callback for memory management and checkpoint cleanup\n",
        "\n",
        "    Features:\n",
        "    - Periodic memory cleanup during training\n",
        "    - Automatic removal of old checkpoints (keeps only latest)\n",
        "    - Memory cleanup before evaluation\n",
        "    \"\"\"\n",
        "\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        \"\"\"Clean memory after each training step\"\"\"\n",
        "        if state.global_step % 10 == 0:  # Every 10 steps\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "        return control\n",
        "\n",
        "    def on_save(self, args, state, control, **kwargs):\n",
        "        \"\"\"Clean up old checkpoints after saving\"\"\"\n",
        "        # Remove old checkpoints except the latest\n",
        "        if os.path.exists(args.output_dir):\n",
        "            checkpoints = [d for d in os.listdir(args.output_dir) if d.startswith(\"checkpoint-\")]\n",
        "            if len(checkpoints) > 1:\n",
        "                # Sort by checkpoint number\n",
        "                checkpoints_sorted = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
        "                # Keep only the last checkpoint\n",
        "                for checkpoint in checkpoints_sorted[:-1]:\n",
        "                    checkpoint_path = os.path.join(args.output_dir, checkpoint)\n",
        "                    try:\n",
        "                        shutil.rmtree(checkpoint_path)\n",
        "                        print(f\"ğŸ—‘ï¸ Removed old checkpoint: {checkpoint}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"âš ï¸ Could not remove {checkpoint}: {e}\")\n",
        "\n",
        "        # Clean memory after saving\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        return control\n",
        "\n",
        "    def on_evaluate(self, args, state, control, **kwargs):\n",
        "        \"\"\"Clean memory before evaluation\"\"\"\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        return control\n",
        "\n",
        "memory_callback = MemoryCleanupCallback()\n",
        "print(\"âœ… Memory cleanup callback created with checkpoint management\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15825824",
      "metadata": {
        "id": "15825824",
        "papermill": {
          "duration": 0.036754,
          "end_time": "2025-11-17T07:56:57.047196",
          "exception": false,
          "start_time": "2025-11-17T07:56:57.010442",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# TRAINER INITIALIZATION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28f6ab69",
      "metadata": {
        "id": "28f6ab69",
        "papermill": {
          "duration": 0.031452,
          "end_time": "2025-11-17T07:56:57.110497",
          "exception": false,
          "start_time": "2025-11-17T07:56:57.079045",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Initialize Seq2SeqTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f9f107c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:56:57.173981Z",
          "iopub.status.busy": "2025-11-17T07:56:57.173720Z",
          "iopub.status.idle": "2025-11-17T07:56:58.670695Z",
          "shell.execute_reply": "2025-11-17T07:56:58.669936Z"
        },
        "id": "7f9f107c",
        "papermill": {
          "duration": 1.530187,
          "end_time": "2025-11-17T07:56:58.671801",
          "exception": false,
          "start_time": "2025-11-17T07:56:57.141614",
          "status": "completed"
        },
        "tags": [],
        "outputId": "aa5007dd-29b9-45d6-b22f-97d0d5cd95d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "INITIALIZING TRAINER\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Trainer initialized successfully\n",
            "   - Model: Whisper Medium (Bengali Regional)\n",
            "   - Training samples: 3015\n",
            "   - Evaluation samples: 335\n",
            "   - Memory optimization: Enabled\n",
            "   - Checkpoint management: Enabled\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"INITIALIZING TRAINER\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=custom_voice[\"train\"],\n",
        "    eval_dataset=custom_voice[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    callbacks=[memory_callback],\n",
        ")\n",
        "\n",
        "print(\"âœ… Trainer initialized successfully\")\n",
        "print(\"   - Model: Whisper Medium (Bengali Regional)\")\n",
        "print(\"   - Training samples:\", len(custom_voice[\"train\"]))\n",
        "print(\"   - Evaluation samples:\", len(custom_voice[\"test\"]))\n",
        "print(\"   - Memory optimization: Enabled\")\n",
        "print(\"   - Checkpoint management: Enabled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40d2057c",
      "metadata": {
        "id": "40d2057c",
        "papermill": {
          "duration": 0.032184,
          "end_time": "2025-11-17T07:56:58.736801",
          "exception": false,
          "start_time": "2025-11-17T07:56:58.704617",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Save Processor Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baca9d18",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:56:58.809569Z",
          "iopub.status.busy": "2025-11-17T07:56:58.809294Z",
          "iopub.status.idle": "2025-11-17T07:56:59.021746Z",
          "shell.execute_reply": "2025-11-17T07:56:59.021016Z"
        },
        "id": "baca9d18",
        "papermill": {
          "duration": 0.251975,
          "end_time": "2025-11-17T07:56:59.023052",
          "exception": false,
          "start_time": "2025-11-17T07:56:58.771077",
          "status": "completed"
        },
        "tags": [],
        "outputId": "bb5beaf0-d04e-44fc-a59a-1e30a905542a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ’¾ Saving processor configuration...\n",
            "âœ… Processor saved to: ./whisper-medium-bengali-regional\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nğŸ’¾ Saving processor configuration...\")\n",
        "processor.save_pretrained(training_args.output_dir)\n",
        "print(f\"âœ… Processor saved to: {training_args.output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6c61092",
      "metadata": {
        "id": "b6c61092",
        "papermill": {
          "duration": 0.03337,
          "end_time": "2025-11-17T07:56:59.097180",
          "exception": false,
          "start_time": "2025-11-17T07:56:59.063810",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# TRAINING EXECUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d10e2e72",
      "metadata": {
        "id": "d10e2e72",
        "papermill": {
          "duration": 0.03303,
          "end_time": "2025-11-17T07:56:59.163651",
          "exception": false,
          "start_time": "2025-11-17T07:56:59.130621",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Display Training Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b5cc830",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:56:59.236982Z",
          "iopub.status.busy": "2025-11-17T07:56:59.236512Z",
          "iopub.status.idle": "2025-11-17T07:56:59.242848Z",
          "shell.execute_reply": "2025-11-17T07:56:59.242160Z"
        },
        "id": "0b5cc830",
        "papermill": {
          "duration": 0.041109,
          "end_time": "2025-11-17T07:56:59.243884",
          "exception": false,
          "start_time": "2025-11-17T07:56:59.202775",
          "status": "completed"
        },
        "tags": [],
        "outputId": "59575faf-ec82-4124-89df-be55107dfafd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "TRAINING SUMMARY\n",
            "======================================================================\n",
            "\n",
            "ğŸ“Š Dataset Statistics:\n",
            "   Training samples: 3015\n",
            "   Evaluation samples: 335\n",
            "\n",
            "âš™ï¸ Training Configuration:\n",
            "   Per-device batch size: 4\n",
            "   Gradient accumulation steps: 4\n",
            "   Effective batch size: 16\n",
            "   Learning rate: 1e-05\n",
            "   Total training steps: 4000\n",
            "   Evaluation frequency: Every 1000 steps\n",
            "   Checkpoint limit: 1 (latest only)\n",
            "\n",
            "â±ï¸ Estimated Training Time:\n",
            "   Approximate duration: 3.5-4 hours\n",
            "   Evaluations: 4 times\n",
            "\n",
            "ğŸ’¾ Memory Management:\n",
            "   FP16 training: Enabled\n",
            "   Gradient checkpointing: Enabled\n",
            "   Automatic checkpoint cleanup: Enabled\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TRAINING SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nğŸ“Š Dataset Statistics:\")\n",
        "print(f\"   Training samples: {len(custom_voice['train'])}\")\n",
        "print(f\"   Evaluation samples: {len(custom_voice['test'])}\")\n",
        "\n",
        "print(f\"\\nâš™ï¸ Training Configuration:\")\n",
        "print(f\"   Per-device batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"   Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"   Total training steps: {training_args.max_steps}\")\n",
        "print(f\"   Evaluation frequency: Every {training_args.eval_steps} steps\")\n",
        "print(f\"   Checkpoint limit: {training_args.save_total_limit} (latest only)\")\n",
        "\n",
        "print(f\"\\nâ±ï¸ Estimated Training Time:\")\n",
        "print(f\"   Approximate duration: 3.5-4 hours\")\n",
        "print(f\"   Evaluations: {training_args.max_steps // training_args.eval_steps} times\")\n",
        "\n",
        "print(f\"\\nğŸ’¾ Memory Management:\")\n",
        "print(f\"   FP16 training: Enabled\")\n",
        "print(f\"   Gradient checkpointing: Enabled\")\n",
        "print(f\"   Automatic checkpoint cleanup: Enabled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1a54855",
      "metadata": {
        "id": "c1a54855",
        "papermill": {
          "duration": 0.032354,
          "end_time": "2025-11-17T07:56:59.309922",
          "exception": false,
          "start_time": "2025-11-17T07:56:59.277568",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Configure Memory Allocation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8589c771",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:56:59.377875Z",
          "iopub.status.busy": "2025-11-17T07:56:59.377605Z",
          "iopub.status.idle": "2025-11-17T07:56:59.828772Z",
          "shell.execute_reply": "2025-11-17T07:56:59.828114Z"
        },
        "id": "8589c771",
        "papermill": {
          "duration": 0.485989,
          "end_time": "2025-11-17T07:56:59.829839",
          "exception": false,
          "start_time": "2025-11-17T07:56:59.343850",
          "status": "completed"
        },
        "tags": [],
        "outputId": "249174fc-8f73-44f6-a0d7-55451f1f2cb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ”§ Configuring memory allocation...\n",
            "   Initial GPU Memory: 2.85 GB\n",
            "   GPU Memory Reserved: 2.86 GB\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nğŸ”§ Configuring memory allocation...\")\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(f\"   Initial GPU Memory: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
        "    print(f\"   GPU Memory Reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ad86dba",
      "metadata": {
        "id": "2ad86dba",
        "papermill": {
          "duration": 0.048621,
          "end_time": "2025-11-17T07:56:59.913096",
          "exception": false,
          "start_time": "2025-11-17T07:56:59.864475",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## START TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fba7d01b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T07:56:59.980151Z",
          "iopub.status.busy": "2025-11-17T07:56:59.979861Z",
          "iopub.status.idle": "2025-11-17T16:31:07.623648Z",
          "shell.execute_reply": "2025-11-17T16:31:07.622844Z"
        },
        "id": "fba7d01b",
        "papermill": {
          "duration": 30847.678675,
          "end_time": "2025-11-17T16:31:07.624908",
          "exception": false,
          "start_time": "2025-11-17T07:56:59.946233",
          "status": "completed"
        },
        "tags": [],
        "outputId": "c82ed733-4fda-4025-88c3-58015286b17a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "ğŸš€ STARTING TRAINING\n",
            "======================================================================\n",
            "\n",
            "â³ Training in progress... This will take approximately 3.5-4 hours\n",
            "ğŸ“Š You can monitor progress in TensorBoard\n",
            "ğŸ’¡ Tip: Keep this tab open to track training progress\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4000/4000 8:33:57, Epoch 21/22]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.008100</td>\n",
              "      <td>0.064153</td>\n",
              "      <td>10.964187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.055127</td>\n",
              "      <td>7.107438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.057683</td>\n",
              "      <td>7.878788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.050029</td>\n",
              "      <td>6.170799</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ—‘ï¸ Removed old checkpoint: checkpoint-1000\n",
            "ğŸ—‘ï¸ Removed old checkpoint: checkpoint-2000\n",
            "ğŸ—‘ï¸ Removed old checkpoint: checkpoint-3000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "âœ… TRAINING COMPLETED SUCCESSFULLY!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ğŸš€ STARTING TRAINING\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "print(\"â³ Training in progress... This will take approximately 3.5-4 hours\")\n",
        "print(\"ğŸ“Š You can monitor progress in TensorBoard\")\n",
        "print(\"ğŸ’¡ Tip: Keep this tab open to track training progress\\n\")\n",
        "\n",
        "try:\n",
        "    # Start training\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"âœ… TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "except Exception as e:\n",
        "    error_msg = str(e)\n",
        "    print(f\"\\n{'=' * 70}\")\n",
        "    print(\"âŒ TRAINING ERROR\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\nError: {error_msg}\\n\")\n",
        "\n",
        "    # Provide specific solutions based on error type\n",
        "    if \"out of memory\" in error_msg.lower():\n",
        "        print(\"ğŸ”§ SOLUTION FOR OUT OF MEMORY ERROR:\")\n",
        "        print(\"   Reduce batch size in Cell 11.1:\")\n",
        "        print(\"   Change: per_device_train_batch_size=2\")\n",
        "        print(\"   Change: gradient_accumulation_steps=8\")\n",
        "        print(\"   This maintains effective batch size while using less memory\\n\")\n",
        "\n",
        "    elif \"unscale\" in error_msg.lower() or \"fp16\" in error_msg.lower():\n",
        "        print(\"ğŸ”§ SOLUTION FOR FP16 ERROR:\")\n",
        "        print(\"   This error shouldn't occur with the current configuration\")\n",
        "        print(\"   If it persists, try setting fp16=False in Cell 11.1\\n\")\n",
        "\n",
        "    # Print full traceback\n",
        "    import traceback\n",
        "    print(\"Full error traceback:\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "    # Clean up memory\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    raise  # Re-raise the exception to stop execution"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68d569ac",
      "metadata": {
        "id": "68d569ac",
        "papermill": {
          "duration": 0.034924,
          "end_time": "2025-11-17T16:31:07.695007",
          "exception": false,
          "start_time": "2025-11-17T16:31:07.660083",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# MODEL SAVING"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5637847c",
      "metadata": {
        "id": "5637847c",
        "papermill": {
          "duration": 0.037746,
          "end_time": "2025-11-17T16:31:07.766442",
          "exception": false,
          "start_time": "2025-11-17T16:31:07.728696",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Pre-Save Memory Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af964ab3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T16:31:07.838382Z",
          "iopub.status.busy": "2025-11-17T16:31:07.837861Z",
          "iopub.status.idle": "2025-11-17T16:31:08.276382Z",
          "shell.execute_reply": "2025-11-17T16:31:08.275570Z"
        },
        "id": "af964ab3",
        "papermill": {
          "duration": 0.47766,
          "end_time": "2025-11-17T16:31:08.277549",
          "exception": false,
          "start_time": "2025-11-17T16:31:07.799889",
          "status": "completed"
        },
        "tags": [],
        "outputId": "233b3d6a-46e7-41e6-addb-76f2637d8991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "SAVING FINAL MODEL\n",
            "======================================================================\n",
            "\n",
            "ğŸ§¹ Cleaning up memory before saving...\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"SAVING FINAL MODEL\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "print(\"ğŸ§¹ Cleaning up memory before saving...\")\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c67a62e",
      "metadata": {
        "id": "0c67a62e",
        "papermill": {
          "duration": 0.041229,
          "end_time": "2025-11-17T16:31:08.353935",
          "exception": false,
          "start_time": "2025-11-17T16:31:08.312706",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Save Final Model and Processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "041a87cf",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T16:31:08.424387Z",
          "iopub.status.busy": "2025-11-17T16:31:08.423881Z",
          "iopub.status.idle": "2025-11-17T16:31:31.869154Z",
          "shell.execute_reply": "2025-11-17T16:31:31.868272Z"
        },
        "id": "041a87cf",
        "papermill": {
          "duration": 23.479891,
          "end_time": "2025-11-17T16:31:31.870639",
          "exception": false,
          "start_time": "2025-11-17T16:31:08.390748",
          "status": "completed"
        },
        "tags": [],
        "outputId": "5cd91f33-35e5-4d86-bc8c-cd6df42028c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ’¾ Saving final model and processor...\n",
            "âœ… Model saved successfully to: ./whisper-medium-bengali-regional-final\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ’¾ Saving final model and processor...\")\n",
        "\n",
        "final_model_path = \"./whisper-medium-bengali-regional-final\"\n",
        "\n",
        "trainer.save_model(final_model_path)\n",
        "processor.save_pretrained(final_model_path)\n",
        "\n",
        "print(f\"âœ… Model saved successfully to: {final_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27fa7a2a",
      "metadata": {
        "id": "27fa7a2a",
        "papermill": {
          "duration": 0.049,
          "end_time": "2025-11-17T16:31:31.994656",
          "exception": false,
          "start_time": "2025-11-17T16:31:31.945656",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Clean Up Training Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "911b9199",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T16:31:32.060980Z",
          "iopub.status.busy": "2025-11-17T16:31:32.060707Z",
          "iopub.status.idle": "2025-11-17T16:31:33.444243Z",
          "shell.execute_reply": "2025-11-17T16:31:33.443394Z"
        },
        "id": "911b9199",
        "papermill": {
          "duration": 1.419353,
          "end_time": "2025-11-17T16:31:33.446202",
          "exception": false,
          "start_time": "2025-11-17T16:31:32.026849",
          "status": "completed"
        },
        "tags": [],
        "outputId": "fb8a5313-d93b-4303-9907-61db8c16bf04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ—‘ï¸ Cleaning up intermediate checkpoints...\n",
            "   âœ“ Removed: checkpoint-4000\n",
            "\n",
            "âœ… Removed 1 intermediate checkpoint(s)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nğŸ—‘ï¸ Cleaning up intermediate checkpoints...\")\n",
        "\n",
        "checkpoint_count = 0\n",
        "if os.path.exists(training_args.output_dir):\n",
        "    checkpoints = [d for d in os.listdir(training_args.output_dir)\n",
        "                   if d.startswith(\"checkpoint-\")]\n",
        "\n",
        "    for checkpoint in checkpoints:\n",
        "        checkpoint_path = os.path.join(training_args.output_dir, checkpoint)\n",
        "        try:\n",
        "            shutil.rmtree(checkpoint_path)\n",
        "            checkpoint_count += 1\n",
        "            print(f\"   âœ“ Removed: {checkpoint}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   âœ— Could not remove {checkpoint}: {e}\")\n",
        "\n",
        "if checkpoint_count > 0:\n",
        "    print(f\"\\nâœ… Removed {checkpoint_count} intermediate checkpoint(s)\")\n",
        "else:\n",
        "    print(\"\\nâœ“ No intermediate checkpoints to remove\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "169fc9df",
      "metadata": {
        "id": "169fc9df",
        "papermill": {
          "duration": 0.034436,
          "end_time": "2025-11-17T16:31:33.524595",
          "exception": false,
          "start_time": "2025-11-17T16:31:33.490159",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# TRAINING RESULTS SUMMARY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54c4388c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-17T16:31:33.592262Z",
          "iopub.status.busy": "2025-11-17T16:31:33.591933Z",
          "iopub.status.idle": "2025-11-17T16:31:33.598029Z",
          "shell.execute_reply": "2025-11-17T16:31:33.597429Z"
        },
        "id": "54c4388c",
        "papermill": {
          "duration": 0.040763,
          "end_time": "2025-11-17T16:31:33.599088",
          "exception": false,
          "start_time": "2025-11-17T16:31:33.558325",
          "status": "completed"
        },
        "tags": [],
        "outputId": "c99b6115-2a57-4dc0-a324-bf71f60ca9f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "TRAINING RESULTS SUMMARY\n",
            "======================================================================\n",
            "\n",
            "âœ… Training completed successfully!\n",
            "\n",
            "ğŸ“Š Final Model Information:\n",
            "   Model path: ./whisper-medium-bengali-regional-final\n",
            "   Architecture: Whisper Medium\n",
            "   Language: Bengali (20 regional dialects)\n",
            "   Training samples: 3015\n",
            "   Evaluation samples: 335\n",
            "   Training steps completed: 4000\n",
            "\n",
            "ğŸ’¾ Saved Components:\n",
            "   âœ“ Model weights\n",
            "   âœ“ Model configuration\n",
            "   âœ“ Processor (tokenizer + feature extractor)\n",
            "   âœ“ Generation config\n",
            "\n",
            "ğŸ“ˆ Evaluation Metrics:\n",
            "   Check the final WER (Word Error Rate) score above\n",
            "   Lower WER = better performance\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TRAINING RESULTS SUMMARY\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "print(\"âœ… Training completed successfully!\\n\")\n",
        "\n",
        "print(\"ğŸ“Š Final Model Information:\")\n",
        "print(f\"   Model path: {final_model_path}\")\n",
        "print(f\"   Architecture: Whisper Medium\")\n",
        "print(f\"   Language: Bengali (20 regional dialects)\")\n",
        "print(f\"   Training samples: {len(custom_voice['train'])}\")\n",
        "print(f\"   Evaluation samples: {len(custom_voice['test'])}\")\n",
        "print(f\"   Training steps completed: {training_args.max_steps}\")\n",
        "\n",
        "print(\"\\nğŸ’¾ Saved Components:\")\n",
        "print(f\"   âœ“ Model weights\")\n",
        "print(f\"   âœ“ Model configuration\")\n",
        "print(f\"   âœ“ Processor (tokenizer + feature extractor)\")\n",
        "print(f\"   âœ“ Generation config\")\n",
        "\n",
        "print(\"\\nğŸ“ˆ Evaluation Metrics:\")\n",
        "print(\"   Check the final WER (Word Error Rate) score above\")\n",
        "print(\"   Lower WER = better performance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7909cb4",
      "metadata": {
        "id": "e7909cb4",
        "papermill": {
          "duration": 0.03239,
          "end_time": "2025-11-17T16:31:33.664085",
          "exception": false,
          "start_time": "2025-11-17T16:31:33.631695",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# END OF TRAINING"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31192,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 31167.011382,
      "end_time": "2025-11-17T16:32:19.220735",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-11-17T07:52:52.209353",
      "version": "2.6.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}